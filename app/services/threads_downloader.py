"""Threads (Meta) ä¸²æ–‡ä¸‹è¼‰æœå‹™"""

import asyncio
import http.cookiejar
import logging
import re
import subprocess
import uuid
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Optional, List

import requests

from app.config import settings


logger = logging.getLogger(__name__)


# Cookies æª”æ¡ˆè·¯å¾‘
COOKIES_FILE_PATH = Path(__file__).parent.parent.parent / "cookies.txt"


@dataclass
class ThreadsMedia:
    """Threads åª’é«”è³‡æ–™"""

    url: str
    media_type: str  # "image" | "video"


@dataclass
class ThreadsMediaDownloadResult:
    """Threads åª’é«”ä¸‹è¼‰çµæœ"""

    success: bool
    image_paths: List[Path] = field(default_factory=list)
    video_paths: List[Path] = field(default_factory=list)
    audio_paths: List[Path] = field(default_factory=list)  # å¾å½±ç‰‡ä¸­æå–çš„éŸ³è¨Š
    error_message: Optional[str] = None


@dataclass
class ThreadPost:
    """Threads è²¼æ–‡è³‡æ–™"""

    id: str
    author_username: str
    text_content: str
    timestamp: Optional[datetime] = None
    like_count: int = 0
    reply_count: int = 0
    media: List[ThreadsMedia] = field(default_factory=list)  # åª’é«”åˆ—è¡¨ï¼ˆå«é¡å‹ï¼‰
    quoted_post: Optional["ThreadPost"] = None


@dataclass
class ThreadConversation:
    """Threads å°è©±ä¸²è³‡æ–™"""

    parent_post: ThreadPost
    replies: List[ThreadPost] = field(default_factory=list)


@dataclass
class ThreadsDownloadResult:
    """Threads ä¸‹è¼‰çµæœ"""

    success: bool
    content_type: str = "single_post"  # "single_post" | "thread_conversation"
    post: Optional[ThreadPost] = None
    conversation: Optional[ThreadConversation] = None
    error_message: Optional[str] = None


class ThreadsDownloader:
    """Threads (Meta) ä¸²æ–‡ä¸‹è¼‰å™¨"""

    # æ”¯æ´çš„ Threads URL æ ¼å¼
    # https://www.threads.net/@username/post/ABC123xyz
    # https://www.threads.com/@username/post/ABC123xyz
    # https://threads.net/t/ABC123xyz
    THREADS_URL_PATTERNS = [
        r"https?://(?:www\.)?threads\.(?:net|com)/@([\w.]+)/post/([A-Za-z0-9_-]+)",
        r"https?://(?:www\.)?threads\.(?:net|com)/t/([A-Za-z0-9_-]+)",
    ]

    def __init__(self):
        self._api = None
        self._logged_in = False

    def _load_cookies_from_file(self) -> dict:
        """
        å¾ Netscape cookie æª”æ¡ˆè¼‰å…¥ cookies
        
        Returns:
            dict: åŒ…å« cookie åç¨±å’Œå€¼çš„å­—å…¸
        """
        cookies = {}
        
        if not COOKIES_FILE_PATH.exists():
            logger.debug(f"Cookie æª”æ¡ˆä¸å­˜åœ¨: {COOKIES_FILE_PATH}")
            return cookies
            
        try:
            # ä½¿ç”¨ http.cookiejar è§£æ Netscape æ ¼å¼
            cookie_jar = http.cookiejar.MozillaCookieJar(str(COOKIES_FILE_PATH))
            cookie_jar.load(ignore_discard=True, ignore_expires=True)
            
            for cookie in cookie_jar:
                # åªè¼‰å…¥ instagram.com å’Œ threads.net çš„ cookies
                if 'instagram.com' in cookie.domain or 'threads.net' in cookie.domain:
                    cookies[cookie.name] = cookie.value
                    
            logger.debug(f"å¾ cookies.txt è¼‰å…¥ {len(cookies)} å€‹ cookies")
            return cookies
            
        except Exception as e:
            logger.warning(f"è¼‰å…¥ cookies å¤±æ•—: {e}")
            return cookies

    def _get_api(self):
        """
        å–å¾— MetaThreads API å¯¦ä¾‹ï¼ˆæ‡¶æƒ°åˆå§‹åŒ–ï¼‰

        Returns:
            ThreadsAPI: MetaThreads API å¯¦ä¾‹
        """
        if self._api is not None:
            return self._api

        try:
            from metathreads import MetaThreads

            self._api = MetaThreads()

            # å„ªå…ˆå˜—è©¦ä½¿ç”¨ cookie èªè­‰
            cookies = self._load_cookies_from_file()
            if cookies:
                try:
                    # å°‡ cookies æ³¨å…¥åˆ° httpx session
                    for name, value in cookies.items():
                        self._api.session.cookies.set(name, value, domain=".threads.net")
                        # ä¹Ÿè¨­å®šåˆ° instagram.comï¼ˆæœ‰äº› API å¯èƒ½éœ€è¦ï¼‰
                        self._api.session.cookies.set(name, value, domain=".instagram.com")
                    
                    self._logged_in = True
                    logger.info(f"âœ… Threads ä½¿ç”¨ cookie èªè­‰æˆåŠŸ (è¼‰å…¥ {len(cookies)} å€‹ cookies)")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ³¨å…¥ cookies å¤±æ•—: {e}")
                    self._logged_in = False
            # è‹¥ç„¡ cookieï¼Œå˜—è©¦å¸³è™Ÿå¯†ç¢¼ç™»å…¥
            elif settings.threads_username and settings.threads_password:
                try:
                    self._api.login(
                        settings.threads_username,
                        settings.threads_password
                    )
                    self._logged_in = True
                    logger.info(f"âœ… Threads ç™»å…¥æˆåŠŸ: @{settings.threads_username}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Threads ç™»å…¥å¤±æ•—: {e}ï¼Œå°‡ä½¿ç”¨æœªèªè­‰æ¨¡å¼")
                    self._logged_in = False
            else:
                logger.info("Threads ä½¿ç”¨æœªèªè­‰æ¨¡å¼ï¼ˆåƒ…èƒ½å­˜å–å…¬é–‹å…§å®¹ï¼‰")

            return self._api

        except ImportError:
            logger.error("âŒ MetaThreads å‡½å¼åº«æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ 'pip install metathreads'")
            raise RuntimeError("MetaThreads å‡½å¼åº«æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ 'pip install metathreads'")

    def _get_session_with_cookies(self) -> requests.Session:
        """
        å»ºç«‹å¸¶æœ‰ cookies çš„ requests Sessionï¼ˆç”¨æ–¼ web scrapingï¼‰
        
        Returns:
            requests.Session: è¨­å®šå¥½ cookies çš„ session
        """
        session = requests.Session()
        
        # è¼‰å…¥ cookies
        if COOKIES_FILE_PATH.exists():
            try:
                cookie_jar = http.cookiejar.MozillaCookieJar(str(COOKIES_FILE_PATH))
                cookie_jar.load(ignore_discard=True, ignore_expires=True)
                for cookie in cookie_jar:
                    session.cookies.set(cookie.name, cookie.value, domain=cookie.domain)
                logger.debug(f"Web scraping: è¼‰å…¥ {len(session.cookies)} å€‹ cookies")
            except Exception as e:
                logger.warning(f"è¼‰å…¥ cookies å¤±æ•—: {e}")
        
        # è¨­å®šç€è¦½å™¨ headers (ç°¡åŒ–ç‰ˆï¼Œé¿å…è§¸ç™¼åçˆ¬èŸ²)
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        })
        
        return session

    def _decode_unicode_text(self, raw_text: str) -> str:
        """
        è§£ç¢¼ unicode è½‰ç¾©åºåˆ—
        
        Args:
            raw_text: åŸå§‹æ–‡å­—ï¼ˆå¯èƒ½åŒ…å« \\uXXXXï¼‰
            
        Returns:
            è§£ç¢¼å¾Œçš„æ–‡å­—
        """
        try:
            # ä½¿ç”¨ latin-1 ç·¨ç¢¼ç„¶å¾Œ unicode-escape è§£ç¢¼
            return raw_text.encode('latin-1').decode('unicode-escape')
        except Exception:
            try:
                # å‚™ç”¨æ–¹æ¡ˆï¼šç›´æ¥ unicode-escape
                return raw_text.encode('utf-8').decode('unicode-escape')
            except Exception:
                # æœ€å¾Œæ–¹æ¡ˆï¼šè¿”å›åŸå§‹æ–‡å­—
                return raw_text

    def _download_via_web_scraping(self, url: str) -> Optional[ThreadPost]:
        """
        é€é Web Scraping ä¸‹è¼‰ Threads è²¼æ–‡ï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰
        æ”¯æ´ä¸²æ–‡ï¼ˆthreadï¼‰- æœƒæŠ“å–åŒä¸€ä½œè€…çš„æ‰€æœ‰é€£çºŒè²¼æ–‡
        
        Args:
            url: Threads è²¼æ–‡ URL
            
        Returns:
            ThreadPost æˆ– None
        """
        logger.info(f"å˜—è©¦ä½¿ç”¨ Web Scraping æŠ“å– Threads è²¼æ–‡...")
        
        try:
            session = self._get_session_with_cookies()
            response = session.get(url, timeout=30)
            response.raise_for_status()
            
            html = response.text
            
            # æå–ä½¿ç”¨è€…åç¨±
            username = self.extract_username(url)
            if not username:
                # å¾ HTML ä¸­æå–
                username_match = re.search(r'"username":"([^"]+)"', html)
                if username_match:
                    username = username_match.group(1)
            
            # æå–è²¼æ–‡ ID
            post_id = self.extract_post_id(url) or ""
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºä¸²æ–‡ï¼ˆthreadï¼‰
            is_thread = 'thread_items' in html
            
            # æ‰¾åˆ°æ‰€æœ‰è²¼æ–‡çš„ codeï¼ˆIDï¼‰ï¼Œç”¨æ–¼è­˜åˆ¥ä¸²æ–‡ä¸­çš„è²¼æ–‡
            all_codes = re.findall(r'"code":"([A-Za-z0-9_-]+)"', html)
            # éæ¿¾æ‰éè²¼æ–‡ codeï¼ˆå¦‚ en_USï¼‰
            post_codes = [c for c in all_codes if len(c) > 5 and not c.startswith('en_')]
            # å–å¾—ä¸»è²¼æ–‡ ID çš„å‰ç¶´ï¼ˆä¸²æ–‡ä¸­çš„è²¼æ–‡é€šå¸¸æœ‰ç›¸ä¼¼çš„ ID å‰ç¶´ï¼‰
            main_prefix = post_id[:4] if post_id else ""
            # ä¸²æ–‡ä¸­çš„è²¼æ–‡ code
            thread_codes = set(c for c in post_codes if c.startswith(main_prefix)) if main_prefix else set()
            
            logger.debug(f"Web scraping: ä¸»è²¼æ–‡ ID={post_id}, ä¸²æ–‡è²¼æ–‡æ•¸={len(thread_codes)}")
            
            # æå–æ‰€æœ‰æ–‡å­—å…§å®¹
            text_matches = re.findall(r'"text":"((?:[^"\\]|\\.)*?)"', html)
            logger.debug(f"Web scraping: æ‰¾åˆ° {len(text_matches)} å€‹ text æ¬„ä½")
            
            all_texts = []
            seen_texts = set()  # ç”¨æ–¼å»é‡
            
            for raw_text in text_matches:
                if len(raw_text) < 10:  # å¿½ç•¥å¤ªçŸ­çš„æ–‡å­—
                    continue
                    
                decoded = self._decode_unicode_text(raw_text)
                
                # å»é‡ï¼ˆé¿å…é‡è¤‡çš„æ–‡å­—ï¼‰
                text_hash = decoded[:50] if len(decoded) > 50 else decoded
                if text_hash in seen_texts:
                    continue
                seen_texts.add(text_hash)
                
                # éæ¿¾æ‰éè²¼æ–‡å…§å®¹
                if is_thread and len(thread_codes) > 1:
                    # å°æ–¼ä¸²æ–‡ï¼Œåªæ”¶é›†ï¼š
                    # 1. æœ‰ç·¨è™Ÿçš„æ–‡å­— (1ï¸âƒ£ 2ï¸âƒ£ ç­‰)
                    # 2. ç¬¬ä¸€å‰‡ï¼ˆæœ€é•·ï¼‰çš„è²¼æ–‡
                    # 3. çµå°¾é¡çš„æ–‡å­—ï¼ˆå¦‚æœä½ ä¹Ÿè¦ºå¾—...ï¼‰
                    has_number = any(c in decoded for c in '1ï¸âƒ£2ï¸âƒ£3ï¸âƒ£4ï¸âƒ£5ï¸âƒ£6ï¸âƒ£7ï¸âƒ£8ï¸âƒ£9ï¸âƒ£ğŸ”Ÿ')
                    is_conclusion = decoded.startswith('å¦‚æœä½ ä¹Ÿ') or decoded.startswith('æ­¡è¿')
                    is_main_post = len(decoded) > 200  # ä¸»è²¼æ–‡é€šå¸¸è¼ƒé•·
                    
                    if has_number or is_conclusion or is_main_post:
                        all_texts.append(decoded)
                else:
                    # å–®ä¸€è²¼æ–‡æˆ–ç„¡æ³•è­˜åˆ¥ä¸²æ–‡çµæ§‹
                    all_texts.append(decoded)
            
            # çµ„åˆæ–‡å­—å…§å®¹
            if is_thread and len(all_texts) > 1:
                # ä¸²æ–‡ï¼šæŒ‰é †åºçµ„åˆï¼Œç”¨åˆ†éš”ç·šåˆ†é–‹
                text_content = "\n\n---\n\n".join(all_texts)
                logger.info(f"Web scraping: åµæ¸¬åˆ°ä¸²æ–‡ï¼Œå…± {len(all_texts)} å‰‡è²¼æ–‡")
            elif all_texts:
                # å–®ä¸€è²¼æ–‡ï¼šå–æœ€é•·çš„
                text_content = max(all_texts, key=len)
            else:
                text_content = ""
            
            # å¦‚æœæ–‡å­—å…§å®¹æ˜¯ç©ºçš„ï¼Œå˜—è©¦å…¶ä»–æ–¹æ³•
            if not text_content:
                caption_match = re.search(r'"caption":\s*\{\s*"text":"((?:[^"\\]|\\.)*)"', html)
                if caption_match:
                    text_content = self._decode_unicode_text(caption_match.group(1))
            
            # æå–åª’é«” URL
            media_list: List[ThreadsMedia] = []
            
            # åœ–ç‰‡ URL (scontent CDN)
            img_urls = re.findall(r'"url":"(https://scontent[^"]+)"', html)
            # éæ¿¾æ‰ profile åœ–ç‰‡ï¼ˆé€šå¸¸è¼ƒå°ï¼‰
            img_urls = [u for u in img_urls if 's150x150' not in u and '_s150x150' not in u and 't51.2885' not in u]
            # å»é‡ä¸¦å–æœ€é«˜è§£æåº¦
            seen_base = set()
            for img_url in img_urls:
                # å–åŸºæœ¬ URLï¼ˆå»æ‰è§£æåº¦åƒæ•¸ï¼‰
                base_url = re.sub(r'_e\d+_', '_', img_url.split('?')[0])
                if base_url not in seen_base:
                    seen_base.add(base_url)
                    media_list.append(ThreadsMedia(url=img_url, media_type="image"))
            
            # å½±ç‰‡ URL
            video_urls = re.findall(r'"video_url":"([^"]+)"', html)
            for video_url in video_urls:
                # è§£ç¢¼ URL
                video_url = video_url.replace('\\u0026', '&').replace('\\/', '/')
                media_list.append(ThreadsMedia(url=video_url, media_type="video"))
            
            # ä¹Ÿæª¢æŸ¥ video_versions
            video_version_urls = re.findall(r'"video_versions":\[.*?"url":"([^"]+)"', html)
            for video_url in video_version_urls:
                video_url = video_url.replace('\\u0026', '&').replace('\\/', '/')
                if not any(m.url == video_url for m in media_list):
                    media_list.append(ThreadsMedia(url=video_url, media_type="video"))
            
            # æå–æ™‚é–“æˆ³
            timestamp = None
            taken_at_match = re.search(r'"taken_at":(\d+)', html)
            if taken_at_match:
                try:
                    timestamp = datetime.fromtimestamp(int(taken_at_match.group(1)))
                except:
                    pass
            
            # æå–äº’å‹•æ•¸æ“š
            like_count = 0
            like_match = re.search(r'"like_count":(\d+)', html)
            if like_match:
                like_count = int(like_match.group(1))
            
            reply_count = 0
            reply_match = re.search(r'"reply_count":(\d+)|"direct_reply_count":(\d+)', html)
            if reply_match:
                reply_count = int(reply_match.group(1) or reply_match.group(2) or 0)
            
            if not text_content and not media_list:
                logger.warning("Web scraping: ç„¡æ³•æå–è²¼æ–‡å…§å®¹æˆ–åª’é«”")
                return None
            
            logger.info(f"Web scraping æˆåŠŸ: @{username}, {len(media_list)} å€‹åª’é«”, {len(text_content)} å­—")
            
            return ThreadPost(
                id=post_id,
                author_username=username or "unknown",
                text_content=text_content,
                timestamp=timestamp,
                like_count=like_count,
                reply_count=reply_count,
                media=media_list,
            )
            
        except requests.RequestException as e:
            logger.error(f"Web scraping è«‹æ±‚å¤±æ•—: {e}")
            return None
        except Exception as e:
            logger.error(f"Web scraping è§£æå¤±æ•—: {e}")
            return None

    def validate_url(self, url: str) -> bool:
        """é©—è­‰æ˜¯å¦ç‚ºæœ‰æ•ˆçš„ Threads é€£çµ"""
        for pattern in self.THREADS_URL_PATTERNS:
            if re.match(pattern, url):
                return True
        return False

    def extract_post_id(self, url: str) -> Optional[str]:
        """
        å¾ URL æå–è²¼æ–‡ ID

        Args:
            url: Threads é€£çµ

        Returns:
            è²¼æ–‡ ID æˆ– None
        """
        # æ ¼å¼ 1: https://threads.net/@username/post/ABC123xyz æˆ– threads.com
        match = re.match(
            r"https?://(?:www\.)?threads\.(?:net|com)/@[\w.]+/post/([A-Za-z0-9_-]+)",
            url
        )
        if match:
            return match.group(1)

        # æ ¼å¼ 2: https://threads.net/t/ABC123xyz æˆ– threads.com
        match = re.match(
            r"https?://(?:www\.)?threads\.(?:net|com)/t/([A-Za-z0-9_-]+)",
            url
        )
        if match:
            return match.group(1)

        return None

    def extract_username(self, url: str) -> Optional[str]:
        """
        å¾ URL æå–ä½¿ç”¨è€…åç¨±

        Args:
            url: Threads é€£çµ

        Returns:
            ä½¿ç”¨è€…åç¨±æˆ– None
        """
        match = re.match(
            r"https?://(?:www\.)?threads\.(?:net|com)/@([\w.]+)/post/",
            url
        )
        if match:
            return match.group(1)
        return None

    async def download(self, url: str) -> ThreadsDownloadResult:
        """
        ä¸‹è¼‰ Threads è²¼æ–‡å…§å®¹

        Args:
            url: Threads é€£çµ

        Returns:
            ThreadsDownloadResult: ä¸‹è¼‰çµæœ
        """
        if not self.validate_url(url):
            return ThreadsDownloadResult(
                success=False,
                error_message="ç„¡æ³•è§£ææ­¤é€£çµï¼Œè«‹ç¢ºèªæ˜¯å¦ç‚ºæœ‰æ•ˆçš„ Threads é€£çµ",
            )

        post_id = self.extract_post_id(url)
        if not post_id:
            return ThreadsDownloadResult(
                success=False,
                error_message="ç„¡æ³•å¾é€£çµæå–è²¼æ–‡ ID",
            )

        try:
            # åœ¨åŸ·è¡Œç·’æ± ä¸­åŸ·è¡Œï¼ˆMetaThreads æ˜¯åŒæ­¥çš„ï¼‰
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None, self._download_sync, post_id, url
            )
            return result

        except Exception as e:
            logger.error(f"ä¸‹è¼‰ Threads è²¼æ–‡å¤±æ•—: {e}")
            return ThreadsDownloadResult(
                success=False,
                error_message=f"ä¸‹è¼‰å¤±æ•—: {str(e)}",
            )

    def _download_sync(self, post_id: str, url: str) -> ThreadsDownloadResult:
        """åŒæ­¥ä¸‹è¼‰æ–¹æ³•"""
        api_failed = False
        api_error_message = ""
        
        try:
            api = self._get_api()

            # å–å¾—è²¼æ–‡è³‡æ–™
            logger.info(f"æ­£åœ¨æŠ“å– Threads è²¼æ–‡: {post_id}")

            try:
                post_data = api.get_thread(post_id)
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºéŒ¯èª¤å›æ‡‰
                if isinstance(post_data, dict):
                    if post_data.get("status") == "fail" or post_data.get("message"):
                        error_msg = post_data.get("message", "").lower()
                        if "login" in error_msg:
                            api_failed = True
                            api_error_message = "API éœ€è¦ç™»å…¥"
                        elif "not found" in error_msg or "404" in str(post_data):
                            return ThreadsDownloadResult(
                                success=False,
                                error_message="æ‰¾ä¸åˆ°æ­¤è²¼æ–‡ï¼Œå¯èƒ½å·²è¢«åˆªé™¤æˆ–ç‚ºç§äººå…§å®¹",
                            )
                        else:
                            api_failed = True
                            api_error_message = post_data.get('message', 'æœªçŸ¥éŒ¯èª¤')
                            
            except Exception as e:
                error_msg = str(e).lower()
                if "not found" in error_msg or "404" in error_msg:
                    return ThreadsDownloadResult(
                        success=False,
                        error_message="æ‰¾ä¸åˆ°æ­¤è²¼æ–‡ï¼Œå¯èƒ½å·²è¢«åˆªé™¤æˆ–ç‚ºç§äººå…§å®¹",
                    )
                api_failed = True
                api_error_message = str(e)

            # å¦‚æœ API å¤±æ•—ï¼Œå˜—è©¦ Web Scraping
            if api_failed:
                logger.warning(f"MetaThreads API å¤±æ•— ({api_error_message})ï¼Œå˜—è©¦ Web Scraping...")
                scraped_post = self._download_via_web_scraping(url)
                if scraped_post:
                    logger.info(f"âœ… Web Scraping æˆåŠŸ: @{scraped_post.author_username}")
                    return ThreadsDownloadResult(
                        success=True,
                        content_type="single_post",
                        post=scraped_post,
                    )
                else:
                    return ThreadsDownloadResult(
                        success=False,
                        error_message=f"API å’Œ Web Scraping éƒ½ç„¡æ³•å–å¾—å…§å®¹ã€‚\n\nAPI éŒ¯èª¤: {api_error_message}\n\nè«‹ç¢ºèªï¼š\n1. cookies.txt åŒ…å«æœ‰æ•ˆçš„ Instagram ç™»å…¥è³‡è¨Š\n2. æˆ–åœ¨ .env è¨­å®š THREADS_USERNAME å’Œ THREADS_PASSWORD",
                    )

            # è§£æè²¼æ–‡è³‡æ–™
            parent_post = self._parse_post_data(post_data)

            if not parent_post:
                # API å›å‚³ä½†è§£æå¤±æ•—ï¼Œå˜—è©¦ Web Scraping
                logger.warning("API è³‡æ–™è§£æå¤±æ•—ï¼Œå˜—è©¦ Web Scraping...")
                scraped_post = self._download_via_web_scraping(url)
                if scraped_post:
                    return ThreadsDownloadResult(
                        success=True,
                        content_type="single_post",
                        post=scraped_post,
                    )
                return ThreadsDownloadResult(
                    success=False,
                    error_message="ç„¡æ³•è§£æè²¼æ–‡è³‡æ–™",
                )

            # å¦‚æœå•Ÿç”¨äº†æŠ“å–å›è¦†ï¼Œå˜—è©¦å–å¾—å°è©±ä¸²
            if settings.threads_fetch_replies:
                try:
                    conversation = self._fetch_conversation(api, post_id, parent_post)
                    if conversation and conversation.replies:
                        logger.info(f"æˆåŠŸæŠ“å–å°è©±ä¸²ï¼Œå…± {len(conversation.replies)} å‰‡å›è¦†")
                        return ThreadsDownloadResult(
                            success=True,
                            content_type="thread_conversation",
                            conversation=conversation,
                        )
                except Exception as e:
                    logger.warning(f"æŠ“å–å°è©±ä¸²å¤±æ•—ï¼Œå°‡åªå›å‚³åŸå§‹è²¼æ–‡: {e}")

            # å›å‚³å–®ä¸€è²¼æ–‡
            logger.info(f"æˆåŠŸæŠ“å– Threads è²¼æ–‡: @{parent_post.author_username}")
            return ThreadsDownloadResult(
                success=True,
                content_type="single_post",
                post=parent_post,
            )

        except RuntimeError as e:
            # MetaThreads æœªå®‰è£
            return ThreadsDownloadResult(
                success=False,
                error_message=str(e),
            )
        except Exception as e:
            error_msg = str(e)
            logger.error(f"Threads ä¸‹è¼‰å¤±æ•—: {error_msg}")

            if "rate limit" in error_msg.lower():
                return ThreadsDownloadResult(
                    success=False,
                    error_message="å·²é”åˆ° API è«‹æ±‚é™åˆ¶ï¼Œè«‹ç¨å¾Œå†è©¦",
                )

            return ThreadsDownloadResult(
                success=False,
                error_message=f"ä¸‹è¼‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {error_msg}",
            )

    def _parse_post_data(self, post_data: dict) -> Optional[ThreadPost]:
        """
        è§£æ MetaThreads å›å‚³çš„è²¼æ–‡è³‡æ–™

        Args:
            post_data: MetaThreads API å›å‚³çš„åŸå§‹è³‡æ–™

        Returns:
            ThreadPost æˆ– None
        """
        try:
            # MetaThreads çš„è³‡æ–™çµæ§‹å¯èƒ½åœ¨ä¸åŒç‰ˆæœ¬æœ‰æ‰€ä¸åŒ
            # å˜—è©¦å¤šç¨®æ¬„ä½åç¨±
            post_id = (
                post_data.get("id")
                or post_data.get("pk")
                or post_data.get("code")
                or ""
            )

            # å–å¾—ä½œè€…è³‡è¨Š
            user_data = post_data.get("user", {})
            author_username = (
                user_data.get("username")
                or post_data.get("username")
                or "unknown"
            )

            # å–å¾—æ–‡å­—å…§å®¹
            text_content = (
                post_data.get("caption", {}).get("text")
                if isinstance(post_data.get("caption"), dict)
                else post_data.get("caption")
                or post_data.get("text")
                or post_data.get("text_post_app_info", {}).get("share_info", {}).get("quoted_text")
                or ""
            )

            # å–å¾—æ™‚é–“æˆ³è¨˜
            timestamp = None
            taken_at = post_data.get("taken_at") or post_data.get("created_at")
            if taken_at:
                try:
                    if isinstance(taken_at, (int, float)):
                        timestamp = datetime.fromtimestamp(taken_at)
                    elif isinstance(taken_at, str):
                        timestamp = datetime.fromisoformat(taken_at.replace("Z", "+00:00"))
                except Exception:
                    pass

            # å–å¾—äº’å‹•æ•¸æ“š
            like_count = (
                post_data.get("like_count")
                or post_data.get("likes", {}).get("count")
                or 0
            )
            reply_count = (
                post_data.get("reply_count")
                or post_data.get("text_post_app_info", {}).get("direct_reply_count")
                or 0
            )

            # å–å¾—åª’é«” URLï¼ˆå«é¡å‹åˆ¤æ–·ï¼‰
            media_list: List[ThreadsMedia] = []
            carousel_media = post_data.get("carousel_media", [])
            if carousel_media:
                for media in carousel_media:
                    if media.get("video_versions"):
                        media_list.append(ThreadsMedia(
                            url=media["video_versions"][0]["url"],
                            media_type="video"
                        ))
                    elif media.get("image_versions2", {}).get("candidates"):
                        media_list.append(ThreadsMedia(
                            url=media["image_versions2"]["candidates"][0]["url"],
                            media_type="image"
                        ))
            elif post_data.get("video_versions"):
                media_list.append(ThreadsMedia(
                    url=post_data["video_versions"][0]["url"],
                    media_type="video"
                ))
            elif post_data.get("image_versions2", {}).get("candidates"):
                media_list.append(ThreadsMedia(
                    url=post_data["image_versions2"]["candidates"][0]["url"],
                    media_type="image"
                ))

            # å–å¾—å¼•ç”¨è²¼æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰
            quoted_post = None
            quoted_post_data = post_data.get("text_post_app_info", {}).get("share_info", {}).get("quoted_post")
            if quoted_post_data:
                quoted_post = self._parse_post_data(quoted_post_data)

            return ThreadPost(
                id=str(post_id),
                author_username=author_username,
                text_content=text_content,
                timestamp=timestamp,
                like_count=like_count,
                reply_count=reply_count,
                media=media_list,
                quoted_post=quoted_post,
            )

        except Exception as e:
            logger.warning(f"è§£æè²¼æ–‡è³‡æ–™å¤±æ•—: {e}")
            return None

    def _fetch_conversation(
        self,
        api,
        post_id: str,
        parent_post: ThreadPost
    ) -> Optional[ThreadConversation]:
        """
        å–å¾—å®Œæ•´å°è©±ä¸²ï¼ˆåŒ…å«å›è¦†ï¼‰

        Args:
            api: MetaThreads API å¯¦ä¾‹
            post_id: è²¼æ–‡ ID
            parent_post: çˆ¶è²¼æ–‡

        Returns:
            ThreadConversation æˆ– None
        """
        try:
            # å–å¾—å›è¦†
            replies_data = api.get_thread_replies(post_id)

            if not replies_data:
                return ThreadConversation(parent_post=parent_post, replies=[])

            replies = []
            max_replies = settings.threads_max_replies

            for reply_data in replies_data[:max_replies]:
                reply = self._parse_post_data(reply_data)
                if reply:
                    replies.append(reply)

            return ThreadConversation(
                parent_post=parent_post,
                replies=replies,
            )

        except Exception as e:
            logger.warning(f"å–å¾—å°è©±ä¸²å¤±æ•—: {e}")
            return None

    def format_for_summary(self, result: ThreadsDownloadResult) -> str:
        """
        å°‡ä¸‹è¼‰çµæœæ ¼å¼åŒ–ç‚ºé©åˆ LLM æ‘˜è¦çš„æ–‡å­—

        Args:
            result: ä¸‹è¼‰çµæœ

        Returns:
            æ ¼å¼åŒ–å¾Œçš„æ–‡å­—å…§å®¹
        """
        if not result.success:
            return ""

        lines = []

        if result.content_type == "single_post" and result.post:
            lines.append(self._format_post(result.post, is_main=True))

        elif result.content_type == "thread_conversation" and result.conversation:
            # æ ¼å¼åŒ–ä¸»è²¼æ–‡
            lines.append(self._format_post(result.conversation.parent_post, is_main=True))

            # æ ¼å¼åŒ–å›è¦†
            if result.conversation.replies:
                lines.append("\nã€å°è©±ä¸²å›è¦†ã€‘")
                for i, reply in enumerate(result.conversation.replies, 1):
                    lines.append(f"\n--- å›è¦† #{i} ---")
                    lines.append(self._format_post(reply, is_main=False))

        return "\n".join(lines)

    def _format_post(self, post: ThreadPost, is_main: bool = False) -> str:
        """æ ¼å¼åŒ–å–®ä¸€è²¼æ–‡"""
        lines = []

        if is_main:
            lines.append(f"ã€ä¸»è²¼æ–‡ã€‘ @{post.author_username}")
        else:
            lines.append(f"@{post.author_username}")

        if post.timestamp:
            lines.append(f"ç™¼ä½ˆæ™‚é–“: {post.timestamp.strftime('%Y-%m-%d %H:%M')}")

        lines.append(f"\n{post.text_content}")

        if post.like_count > 0 or post.reply_count > 0:
            stats = []
            if post.like_count > 0:
                stats.append(f"â¤ï¸ {post.like_count}")
            if post.reply_count > 0:
                stats.append(f"ğŸ’¬ {post.reply_count}")
            lines.append(f"\n({' | '.join(stats)})")

        if post.quoted_post:
            lines.append(f"\n> å¼•ç”¨è‡ª @{post.quoted_post.author_username}:")
            lines.append(f"> {post.quoted_post.text_content[:200]}...")

        if post.media:
            image_count = sum(1 for m in post.media if m.media_type == "image")
            video_count = sum(1 for m in post.media if m.media_type == "video")
            media_info = []
            if image_count > 0:
                media_info.append(f"{image_count} å¼µåœ–ç‰‡")
            if video_count > 0:
                media_info.append(f"{video_count} å€‹å½±ç‰‡")
            lines.append(f"\n[é™„ä»¶: {', '.join(media_info)}]")

        return "\n".join(lines)

    # ==================== åª’é«”ä¸‹è¼‰æ–¹æ³• ====================

    def _get_temp_dir(self) -> Path:
        """å–å¾—æš«å­˜ç›®éŒ„"""
        temp_dir = Path(settings.temp_video_dir)
        temp_dir.mkdir(parents=True, exist_ok=True)
        return temp_dir

    def _download_image_sync(self, url: str, retry: int = 2) -> Optional[Path]:
        """
        ä¸‹è¼‰å–®å¼µåœ–ç‰‡ï¼ˆåŒæ­¥æ–¹æ³•ï¼Œå«é‡è©¦æ©Ÿåˆ¶ï¼‰

        Args:
            url: åœ–ç‰‡ URL
            retry: é‡è©¦æ¬¡æ•¸

        Returns:
            åœ–ç‰‡æª”æ¡ˆè·¯å¾‘æˆ– None
        """
        temp_dir = self._get_temp_dir()
        file_id = uuid.uuid4().hex[:8]
        image_path = temp_dir / f"threads_img_{file_id}.jpg"

        for attempt in range(retry + 1):
            try:
                response = requests.get(url, timeout=30, stream=True)
                response.raise_for_status()

                with open(image_path, "wb") as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)

                logger.info(f"âœ… åœ–ç‰‡ä¸‹è¼‰æˆåŠŸ: {image_path.name}")
                return image_path

            except Exception as e:
                if attempt < retry:
                    logger.warning(f"åœ–ç‰‡ä¸‹è¼‰å¤±æ•— (ç¬¬ {attempt + 1} æ¬¡)ï¼Œé‡è©¦ä¸­: {e}")
                else:
                    logger.error(f"åœ–ç‰‡ä¸‹è¼‰å¤±æ•— (å·²é”æœ€å¤§é‡è©¦): {e}")

        return None

    def _download_video_sync(self, url: str, retry: int = 2) -> tuple[Optional[Path], Optional[Path]]:
        """
        ä¸‹è¼‰å½±ç‰‡ä¸¦æå–éŸ³è¨Šï¼ˆåŒæ­¥æ–¹æ³•ï¼Œå«é‡è©¦æ©Ÿåˆ¶ï¼‰

        Args:
            url: å½±ç‰‡ URL
            retry: é‡è©¦æ¬¡æ•¸

        Returns:
            (å½±ç‰‡è·¯å¾‘, éŸ³è¨Šè·¯å¾‘) æˆ– (None, None)
        """
        temp_dir = self._get_temp_dir()
        file_id = uuid.uuid4().hex[:8]
        video_path = temp_dir / f"threads_vid_{file_id}.mp4"
        audio_path = temp_dir / f"threads_aud_{file_id}.mp3"

        for attempt in range(retry + 1):
            try:
                # ä¸‹è¼‰å½±ç‰‡
                response = requests.get(url, timeout=60, stream=True)
                response.raise_for_status()

                with open(video_path, "wb") as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)

                logger.info(f"âœ… å½±ç‰‡ä¸‹è¼‰æˆåŠŸ: {video_path.name}")

                # ä½¿ç”¨ ffmpeg æå–éŸ³è¨Šï¼ˆå¦‚æœæœ‰éŸ³è»Œï¼‰
                if self._has_audio_track(video_path):
                    try:
                        subprocess.run(
                            [
                                "ffmpeg", "-y", "-i", str(video_path),
                                "-vn", "-acodec", "libmp3lame", "-q:a", "2",
                                str(audio_path)
                            ],
                            capture_output=True,
                            timeout=60,
                        )
                        if audio_path.exists():
                            logger.info(f"âœ… éŸ³è¨Šæå–æˆåŠŸ: {audio_path.name}")
                            return video_path, audio_path
                    except Exception as e:
                        logger.warning(f"éŸ³è¨Šæå–å¤±æ•—: {e}")

                return video_path, None

            except Exception as e:
                if attempt < retry:
                    logger.warning(f"å½±ç‰‡ä¸‹è¼‰å¤±æ•— (ç¬¬ {attempt + 1} æ¬¡)ï¼Œé‡è©¦ä¸­: {e}")
                else:
                    logger.error(f"å½±ç‰‡ä¸‹è¼‰å¤±æ•— (å·²é”æœ€å¤§é‡è©¦): {e}")

        return None, None

    def _has_audio_track(self, video_path: Path) -> bool:
        """
        ä½¿ç”¨ ffprobe æª¢æ¸¬å½±ç‰‡æ˜¯å¦å«æœ‰éŸ³è»Œ

        Args:
            video_path: å½±ç‰‡æª”æ¡ˆè·¯å¾‘

        Returns:
            True è¡¨ç¤ºæœ‰éŸ³è»Œï¼ŒFalse è¡¨ç¤ºç„¡éŸ³è»Œ
        """
        try:
            result = subprocess.run(
                [
                    "ffprobe", "-v", "error",
                    "-select_streams", "a",
                    "-show_entries", "stream=codec_type",
                    "-of", "csv=p=0",
                    str(video_path)
                ],
                capture_output=True,
                text=True,
                timeout=10,
            )
            has_audio = "audio" in result.stdout.lower()
            logger.debug(f"å½±ç‰‡ {video_path.name} éŸ³è»Œåµæ¸¬: {'æœ‰' if has_audio else 'ç„¡'}")
            return has_audio
        except Exception as e:
            logger.warning(f"éŸ³è»Œåµæ¸¬å¤±æ•—ï¼Œé è¨­ç‚ºæœ‰éŸ³è»Œ: {e}")
            return True  # åµæ¸¬å¤±æ•—æ™‚é è¨­ç‚ºæœ‰éŸ³è»Œï¼ˆæœƒå˜—è©¦è½‰éŒ„ï¼‰

    async def download_media(self, media_list: List[ThreadsMedia]) -> ThreadsMediaDownloadResult:
        """
        ä¸‹è¼‰æ‰€æœ‰åª’é«”æª”æ¡ˆ

        Args:
            media_list: åª’é«”åˆ—è¡¨

        Returns:
            ThreadsMediaDownloadResult: ä¸‹è¼‰çµæœ
        """
        if not media_list:
            return ThreadsMediaDownloadResult(success=True)

        image_paths: List[Path] = []
        video_paths: List[Path] = []
        audio_paths: List[Path] = []

        loop = asyncio.get_event_loop()

        for media in media_list:
            if media.media_type == "image":
                path = await loop.run_in_executor(
                    None, self._download_image_sync, media.url
                )
                if path:
                    image_paths.append(path)

            elif media.media_type == "video":
                video_path, audio_path = await loop.run_in_executor(
                    None, self._download_video_sync, media.url
                )
                if video_path:
                    video_paths.append(video_path)
                if audio_path:
                    audio_paths.append(audio_path)

        # åªè¦æœ‰ä»»ä½•åª’é«”æˆåŠŸä¸‹è¼‰å°±ç®—æˆåŠŸ
        success = len(image_paths) > 0 or len(video_paths) > 0
        error_message = None

        if not success and media_list:
            error_message = "æ‰€æœ‰åª’é«”ä¸‹è¼‰å¤±æ•—"

        logger.info(
            f"åª’é«”ä¸‹è¼‰å®Œæˆ: {len(image_paths)} å¼µåœ–ç‰‡, "
            f"{len(video_paths)} å€‹å½±ç‰‡, {len(audio_paths)} å€‹éŸ³è¨Š"
        )

        return ThreadsMediaDownloadResult(
            success=success,
            image_paths=image_paths,
            video_paths=video_paths,
            audio_paths=audio_paths,
            error_message=error_message,
        )

    def cleanup_media(self, result: ThreadsMediaDownloadResult) -> None:
        """
        æ¸…ç†æš«å­˜çš„åª’é«”æª”æ¡ˆ

        Args:
            result: åª’é«”ä¸‹è¼‰çµæœ
        """
        all_paths = result.image_paths + result.video_paths + result.audio_paths

        for path in all_paths:
            try:
                if path.exists():
                    path.unlink()
                    logger.debug(f"å·²åˆªé™¤æš«å­˜æª”æ¡ˆ: {path.name}")
            except Exception as e:
                logger.warning(f"åˆªé™¤æš«å­˜æª”æ¡ˆå¤±æ•— {path}: {e}")

    def get_all_media(self, result: ThreadsDownloadResult) -> List[ThreadsMedia]:
        """
        å¾ä¸‹è¼‰çµæœä¸­æ”¶é›†æ‰€æœ‰åª’é«”ï¼ˆåŒ…å«ä¸»è²¼æ–‡å’Œå›è¦†ï¼‰

        Args:
            result: Threads ä¸‹è¼‰çµæœ

        Returns:
            æ‰€æœ‰åª’é«”åˆ—è¡¨
        """
        all_media: List[ThreadsMedia] = []

        if result.content_type == "single_post" and result.post:
            all_media.extend(result.post.media)

        elif result.content_type == "thread_conversation" and result.conversation:
            all_media.extend(result.conversation.parent_post.media)
            for reply in result.conversation.replies:
                all_media.extend(reply.media)

        return all_media
