"""Threads (Meta) ä¸²æ–‡ä¸‹è¼‰æœå‹™"""

import asyncio
import http.cookiejar
import logging
import re
import subprocess
import uuid
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Optional, List

import requests

from app.config import settings


logger = logging.getLogger(__name__)


# Cookies æª”æ¡ˆè·¯å¾‘
COOKIES_FILE_PATH = Path(__file__).parent.parent.parent / "cookies.txt"


@dataclass
class ThreadsMedia:
    """Threads åª’é«”è³‡æ–™"""

    url: str
    media_type: str  # "image" | "video"


@dataclass
class ThreadsMediaDownloadResult:
    """Threads åª’é«”ä¸‹è¼‰çµæœ"""

    success: bool
    image_paths: List[Path] = field(default_factory=list)
    video_paths: List[Path] = field(default_factory=list)
    audio_paths: List[Path] = field(default_factory=list)  # å¾å½±ç‰‡ä¸­æå–çš„éŸ³è¨Š
    error_message: Optional[str] = None


@dataclass
class ThreadPost:
    """Threads è²¼æ–‡è³‡æ–™"""

    id: str
    author_username: str
    text_content: str
    timestamp: Optional[datetime] = None
    like_count: int = 0
    reply_count: int = 0
    media: List[ThreadsMedia] = field(default_factory=list)  # åª’é«”åˆ—è¡¨ï¼ˆå«é¡å‹ï¼‰
    quoted_post: Optional["ThreadPost"] = None


@dataclass
class ThreadConversation:
    """Threads å°è©±ä¸²è³‡æ–™"""

    parent_post: ThreadPost
    replies: List[ThreadPost] = field(default_factory=list)


@dataclass
class ThreadsDownloadResult:
    """Threads ä¸‹è¼‰çµæœ"""

    success: bool
    content_type: str = "single_post"  # "single_post" | "thread_conversation" | "thread"
    post: Optional[ThreadPost] = None
    conversation: Optional[ThreadConversation] = None
    thread_posts: List[ThreadPost] = field(default_factory=list)  # ä½œè€…ä¸²æ–‡ï¼ˆå¤šå‰‡é€£çºŒè²¼æ–‡ï¼‰
    error_message: Optional[str] = None


class ThreadsDownloader:
    """Threads (Meta) ä¸²æ–‡ä¸‹è¼‰å™¨"""

    # æ”¯æ´çš„ Threads URL æ ¼å¼
    # https://www.threads.net/@username/post/ABC123xyz
    # https://www.threads.com/@username/post/ABC123xyz
    # https://threads.net/t/ABC123xyz
    THREADS_URL_PATTERNS = [
        r"https?://(?:www\.)?threads\.(?:net|com)/@([\w.]+)/post/([A-Za-z0-9_-]+)",
        r"https?://(?:www\.)?threads\.(?:net|com)/t/([A-Za-z0-9_-]+)",
    ]

    def __init__(self):
        self._api = None
        self._logged_in = False

    def _load_cookies_from_file(self) -> dict:
        """
        å¾ Netscape cookie æª”æ¡ˆè¼‰å…¥ cookies
        
        Returns:
            dict: åŒ…å« cookie åç¨±å’Œå€¼çš„å­—å…¸
        """
        cookies = {}
        
        if not COOKIES_FILE_PATH.exists():
            logger.debug(f"Cookie æª”æ¡ˆä¸å­˜åœ¨: {COOKIES_FILE_PATH}")
            return cookies
            
        try:
            # ä½¿ç”¨ http.cookiejar è§£æ Netscape æ ¼å¼
            cookie_jar = http.cookiejar.MozillaCookieJar(str(COOKIES_FILE_PATH))
            cookie_jar.load(ignore_discard=True, ignore_expires=True)
            
            for cookie in cookie_jar:
                # åªè¼‰å…¥ instagram.com å’Œ threads.net çš„ cookies
                if 'instagram.com' in cookie.domain or 'threads.net' in cookie.domain:
                    cookies[cookie.name] = cookie.value
                    
            logger.debug(f"å¾ cookies.txt è¼‰å…¥ {len(cookies)} å€‹ cookies")
            return cookies
            
        except Exception as e:
            logger.warning(f"è¼‰å…¥ cookies å¤±æ•—: {e}")
            return cookies

    def _get_api(self):
        """
        å–å¾— MetaThreads API å¯¦ä¾‹ï¼ˆæ‡¶æƒ°åˆå§‹åŒ–ï¼‰

        Returns:
            ThreadsAPI: MetaThreads API å¯¦ä¾‹
        """
        if self._api is not None:
            return self._api

        try:
            from metathreads import MetaThreads

            self._api = MetaThreads()

            # å„ªå…ˆå˜—è©¦ä½¿ç”¨ cookie èªè­‰
            cookies = self._load_cookies_from_file()
            if cookies:
                try:
                    # å°‡ cookies æ³¨å…¥åˆ° httpx session
                    for name, value in cookies.items():
                        self._api.session.cookies.set(name, value, domain=".threads.net")
                        # ä¹Ÿè¨­å®šåˆ° instagram.comï¼ˆæœ‰äº› API å¯èƒ½éœ€è¦ï¼‰
                        self._api.session.cookies.set(name, value, domain=".instagram.com")
                    
                    self._logged_in = True
                    logger.info(f"âœ… Threads ä½¿ç”¨ cookie èªè­‰æˆåŠŸ (è¼‰å…¥ {len(cookies)} å€‹ cookies)")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ³¨å…¥ cookies å¤±æ•—: {e}")
                    self._logged_in = False
            # è‹¥ç„¡ cookieï¼Œå˜—è©¦å¸³è™Ÿå¯†ç¢¼ç™»å…¥
            elif settings.threads_username and settings.threads_password:
                try:
                    self._api.login(
                        settings.threads_username,
                        settings.threads_password
                    )
                    self._logged_in = True
                    logger.info(f"âœ… Threads ç™»å…¥æˆåŠŸ: @{settings.threads_username}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Threads ç™»å…¥å¤±æ•—: {e}ï¼Œå°‡ä½¿ç”¨æœªèªè­‰æ¨¡å¼")
                    self._logged_in = False
            else:
                logger.info("Threads ä½¿ç”¨æœªèªè­‰æ¨¡å¼ï¼ˆåƒ…èƒ½å­˜å–å…¬é–‹å…§å®¹ï¼‰")

            return self._api

        except ImportError:
            logger.error("âŒ MetaThreads å‡½å¼åº«æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ 'pip install metathreads'")
            raise RuntimeError("MetaThreads å‡½å¼åº«æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ 'pip install metathreads'")

    def _get_session_with_cookies(self) -> requests.Session:
        """
        å»ºç«‹å¸¶æœ‰ cookies çš„ requests Sessionï¼ˆç”¨æ–¼ web scrapingï¼‰
        
        Returns:
            requests.Session: è¨­å®šå¥½ cookies çš„ session
        """
        session = requests.Session()
        
        # è¼‰å…¥ cookies
        if COOKIES_FILE_PATH.exists():
            try:
                cookie_jar = http.cookiejar.MozillaCookieJar(str(COOKIES_FILE_PATH))
                cookie_jar.load(ignore_discard=True, ignore_expires=True)
                for cookie in cookie_jar:
                    session.cookies.set(cookie.name, cookie.value, domain=cookie.domain)
                logger.debug(f"Web scraping: è¼‰å…¥ {len(session.cookies)} å€‹ cookies")
            except Exception as e:
                logger.warning(f"è¼‰å…¥ cookies å¤±æ•—: {e}")
        
        # è¨­å®šç€è¦½å™¨ headers (ç°¡åŒ–ç‰ˆï¼Œé¿å…è§¸ç™¼åçˆ¬èŸ²)
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        })
        
        return session

    def _decode_unicode_text(self, raw_text: str) -> str:
        """
        è§£ç¢¼ unicode è½‰ç¾©åºåˆ—
        
        Args:
            raw_text: åŸå§‹æ–‡å­—ï¼ˆå¯èƒ½åŒ…å« \\uXXXXï¼‰
            
        Returns:
            è§£ç¢¼å¾Œçš„æ–‡å­—
        """
        try:
            # ä½¿ç”¨ latin-1 ç·¨ç¢¼ç„¶å¾Œ unicode-escape è§£ç¢¼
            decoded = raw_text.encode('latin-1').decode('unicode-escape')
        except Exception:
            try:
                # å‚™ç”¨æ–¹æ¡ˆï¼šç›´æ¥ unicode-escape
                decoded = raw_text.encode('utf-8').decode('unicode-escape')
            except Exception:
                # æœ€å¾Œæ–¹æ¡ˆï¼šè¿”å›åŸå§‹æ–‡å­—
                return raw_text
        
        # æ¸…é™¤ surrogate charactersï¼ˆemoji çš„ surrogate pairs åœ¨ unicode-escape è§£ç¢¼å¾Œ
        # å¯èƒ½ç”¢ç”Ÿç„¡æ•ˆçš„ surrogate å­—å…ƒï¼Œéœ€è¦è½‰æ›ç‚ºæ­£ç¢ºçš„ UTF-8ï¼‰
        try:
            decoded = decoded.encode('utf-16', 'surrogatepass').decode('utf-16')
        except Exception:
            decoded = decoded.encode('utf-8', 'replace').decode('utf-8')
        return decoded

    def _download_via_web_scraping(self, url: str) -> Optional[ThreadPost]:
        """
        é€é Web Scraping ä¸‹è¼‰ Threads è²¼æ–‡ï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰
        æ”¯æ´ä¸²æ–‡ï¼ˆthreadï¼‰- æœƒæŠ“å–åŒä¸€ä½œè€…çš„æ‰€æœ‰é€£çºŒè²¼æ–‡
        
        Args:
            url: Threads è²¼æ–‡ URL
            
        Returns:
            ThreadPost æˆ– None
        """
        logger.info(f"å˜—è©¦ä½¿ç”¨ Web Scraping æŠ“å– Threads è²¼æ–‡...")
        
        try:
            session = self._get_session_with_cookies()
            response = session.get(url, timeout=30)
            response.raise_for_status()
            
            html = response.text
            
            # æå–ä½¿ç”¨è€…åç¨±
            username = self.extract_username(url)
            if not username:
                # å¾ HTML ä¸­æå–
                username_match = re.search(r'"username":"([^"]+)"', html)
                if username_match:
                    username = username_match.group(1)
            
            # æå–è²¼æ–‡ ID
            post_id = self.extract_post_id(url) or ""
            
            logger.debug(f"Web scraping: ä¸»è²¼æ–‡ ID={post_id}")
            
            # æå–æ–‡å­—å…§å®¹ â€” Web scraping åªè™•ç†å–®ä¸€è²¼æ–‡
            # ä¸²æ–‡åµæ¸¬äº¤ç”± Googlebot SSR è™•ç†
            text_content = ""
            
            # å„ªå…ˆå¾ caption æ¬„ä½å–å¾—ï¼ˆæœ€å¯é ï¼‰
            caption_match = re.search(r'"caption":\s*\{\s*"text":"((?:[^"\\]|\\.)*)"', html)
            if caption_match:
                text_content = self._decode_unicode_text(caption_match.group(1))
            
            # å‚™ç”¨ï¼šå¾æ‰€æœ‰ text æ¬„ä½ä¸­å–æœ€é•·çš„
            if not text_content:
                text_matches = re.findall(r'"text":"((?:[^"\\]|\\.)*?)"', html)
                all_texts = []
                seen_texts = set()
                
                for raw_text in text_matches:
                    if len(raw_text) < 10:
                        continue
                    decoded = self._decode_unicode_text(raw_text)
                    text_hash = decoded[:50] if len(decoded) > 50 else decoded
                    if text_hash in seen_texts:
                        continue
                    seen_texts.add(text_hash)
                    all_texts.append(decoded)
                
                if all_texts:
                    text_content = max(all_texts, key=len)
            
            # æå–åª’é«” URL
            media_list: List[ThreadsMedia] = []
            
            # åœ–ç‰‡ URL â€” æ”¯æ´å¤šç¨® CDN åŸŸå
            # 1. scontent CDN (èˆŠç‰ˆ)
            img_urls = re.findall(r'"url":"(https://scontent[^"]+)"', html)
            # 2. instagram.*.fna.fbcdn.net CDN (æ–°ç‰ˆ Threads)
            fbcdn_imgs = re.findall(
                r'(https?://instagram\.[a-z0-9.-]+\.fna\.fbcdn\.net/v/[^\s"\'\\>]+\.(?:jpg|jpeg|png|webp)[^\s"\'\\>]*)',
                html,
            )
            img_urls.extend(fbcdn_imgs)
            # 3. og:image meta tagï¼ˆä½œç‚ºæœ€å¾Œæ‰‹æ®µï¼Œè‡³å°‘å–åˆ°å°é¢åœ–ï¼‰
            if not img_urls:
                og_imgs = re.findall(
                    r'(?:property|name)="og:image"\s+content="([^"]+)"',
                    html,
                )
                for og_url in og_imgs:
                    # HTML entity decode
                    decoded_url = og_url.replace("&amp;", "&")
                    img_urls.append(decoded_url)
            
            # éæ¿¾æ‰ profile åœ–ç‰‡å’Œç¸®åœ–
            img_urls = [
                u for u in img_urls
                if "s150x150" not in u
                and "_s150x150" not in u
                and "t51.2885-19" not in u  # 19 = profile picï¼Œ15 = content image
            ]
            # å»é‡ä¸¦å–æœ€é«˜è§£æåº¦
            seen_base = set()
            for img_url in img_urls:
                # HTML entity decode
                img_url = img_url.replace("&amp;", "&")
                # å–åŸºæœ¬ URLï¼ˆå»æ‰è§£æåº¦åƒæ•¸ï¼‰
                base_url = re.sub(r'_e\d+_', '_', img_url.split('?')[0])
                if base_url not in seen_base:
                    seen_base.add(base_url)
                    media_list.append(ThreadsMedia(url=img_url, media_type="image"))
            
            # å½±ç‰‡ URL
            video_urls = re.findall(r'"video_url":"([^"]+)"', html)
            for video_url in video_urls:
                # è§£ç¢¼ URL
                video_url = video_url.replace('\\u0026', '&').replace('\\/', '/')
                media_list.append(ThreadsMedia(url=video_url, media_type="video"))
            
            # ä¹Ÿæª¢æŸ¥ video_versions
            video_version_urls = re.findall(r'"video_versions":\[.*?"url":"([^"]+)"', html)
            for video_url in video_version_urls:
                video_url = video_url.replace('\\u0026', '&').replace('\\/', '/')
                if not any(m.url == video_url for m in media_list):
                    media_list.append(ThreadsMedia(url=video_url, media_type="video"))
            
            # æå–æ™‚é–“æˆ³
            timestamp = None
            taken_at_match = re.search(r'"taken_at":(\d+)', html)
            if taken_at_match:
                try:
                    timestamp = datetime.fromtimestamp(int(taken_at_match.group(1)))
                except:
                    pass
            
            # æå–äº’å‹•æ•¸æ“š
            like_count = 0
            like_match = re.search(r'"like_count":(\d+)', html)
            if like_match:
                like_count = int(like_match.group(1))
            
            reply_count = 0
            reply_match = re.search(r'"reply_count":(\d+)|"direct_reply_count":(\d+)', html)
            if reply_match:
                reply_count = int(reply_match.group(1) or reply_match.group(2) or 0)
            
            if not text_content and not media_list:
                logger.warning("Web scraping: ç„¡æ³•æå–è²¼æ–‡å…§å®¹æˆ–åª’é«”")
                return None
            
            logger.info(f"Web scraping æˆåŠŸ: @{username}, {len(media_list)} å€‹åª’é«”, {len(text_content)} å­—")
            
            return ThreadPost(
                id=post_id,
                author_username=username or "unknown",
                text_content=text_content,
                timestamp=timestamp,
                like_count=like_count,
                reply_count=reply_count,
                media=media_list,
            )
            
        except requests.RequestException as e:
            logger.error(f"Web scraping è«‹æ±‚å¤±æ•—: {e}")
            return None
        except Exception as e:
            logger.error(f"Web scraping è§£æå¤±æ•—: {e}")
            return None

    # ==================== Googlebot SSR æ–¹æ³• ====================

    def _download_via_googlebot_ssr(self, url: str) -> Optional[ThreadsDownloadResult]:
        """
        é€é Googlebot User-Agent å–å¾— Threads SSR è³‡æ–™ã€‚
        Meta æœƒå° Googlebot å›å‚³ server-side rendered HTMLï¼Œå…¶ä¸­åŒ…å«
        å®Œæ•´çš„ JSON è³‡æ–™ï¼ˆå« thread_items é™£åˆ—ï¼‰ã€‚

        æ­¤æ–¹æ³•å¯æ­£ç¢ºè¾¨è­˜ä½œè€…çš„ä¸²æ–‡ï¼ˆå¤šå‰‡é€£çºŒè²¼æ–‡ï¼‰ï¼Œè‡ªå‹•éæ¿¾
        å…¶ä»–äººçš„å›è¦†ï¼Œåªä¿ç•™åŸä½œè€…çš„è²¼æ–‡ã€‚

        Args:
            url: Threads è²¼æ–‡ URL

        Returns:
            ThreadsDownloadResult æˆ– Noneï¼ˆå¤±æ•—æ™‚ï¼‰
        """
        logger.info("å˜—è©¦ä½¿ç”¨ Googlebot SSR æŠ“å– Threads è²¼æ–‡...")

        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            }
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()

            html = response.text

            if "thread_items" not in html:
                logger.warning("Googlebot SSR: å›æ‡‰ä¸­ç„¡ thread_items")
                return None

            # è§£ææ‰€æœ‰ thread_items é™£åˆ—
            all_thread_posts = self._parse_googlebot_ssr_thread_items(html, url)

            if not all_thread_posts:
                logger.warning("Googlebot SSR: ç„¡æ³•è§£æä»»ä½•è²¼æ–‡")
                return None

            # å–å¾—ä¸»ä½œè€…ï¼ˆç¬¬ä¸€å‰‡è²¼æ–‡çš„ä½œè€…ï¼‰
            main_author = all_thread_posts[0].author_username

            # éæ¿¾ï¼šåªä¿ç•™åŸä½œè€…çš„è²¼æ–‡ï¼ˆæ’é™¤å…¶ä»–äººçš„å›è¦†ï¼‰
            author_posts = [
                p for p in all_thread_posts
                if p.author_username == main_author
            ]

            logger.info(
                f"Googlebot SSR æˆåŠŸ: @{main_author}, "
                f"ä½œè€…è²¼æ–‡ {len(author_posts)}/{len(all_thread_posts)} å‰‡"
            )

            if len(author_posts) == 1:
                # å–®ä¸€è²¼æ–‡
                return ThreadsDownloadResult(
                    success=True,
                    content_type="single_post",
                    post=author_posts[0],
                )
            else:
                # ä¸²æ–‡ï¼ˆå¤šå‰‡é€£çºŒè²¼æ–‡ï¼‰
                return ThreadsDownloadResult(
                    success=True,
                    content_type="thread",
                    thread_posts=author_posts,
                )

        except requests.RequestException as e:
            logger.error(f"Googlebot SSR è«‹æ±‚å¤±æ•—: {e}")
            return None
        except Exception as e:
            logger.error(f"Googlebot SSR è§£æå¤±æ•—: {e}")
            return None

    def _parse_googlebot_ssr_thread_items(
        self, html: str, url: str
    ) -> List[ThreadPost]:
        """
        å¾ Googlebot SSR HTML ä¸­è§£ææ‰€æœ‰ thread_itemsã€‚

        SSR å›æ‡‰çµæ§‹ï¼š
        - edges[0].node.thread_items: ä¸»è²¼æ–‡ï¼ˆ1 å‰‡ï¼‰
        - reply_threads[n].thread_items: å„å›è¦†/çºŒæ–‡ï¼ˆå„ 1 å‰‡ï¼‰

        Args:
            html: Googlebot SSR å›å‚³çš„ HTML
            url: åŸå§‹ URLï¼ˆç”¨æ–¼æå– usernameï¼‰

        Returns:
            æ‰€æœ‰è²¼æ–‡çš„åˆ—è¡¨ï¼ˆæŒ‰å‡ºç¾é †åºï¼‰
        """
        import json as json_mod

        all_posts: List[ThreadPost] = []
        seen_codes: set = set()

        # æå– username ä½œç‚º fallback
        fallback_username = self.extract_username(url) or "unknown"

        # ç­–ç•¥ï¼šæ‰¾å‡ºæ¯å€‹ "thread_items": [...] ä¸¦è§£æå…§å®¹
        # ä½¿ç”¨ JSON è§£ç¢¼å™¨é€ä¸€æŠ½å–
        pattern = re.compile(r'"thread_items":\s*\[')
        for match in pattern.finditer(html):
            start = match.start() + len('"thread_items":')
            # æ‰¾åˆ°é™£åˆ—çš„é–‹é ­ '[' ä½ç½®
            bracket_start = html.index("[", start)

            # ç”¨æ‹¬è™Ÿè¨ˆæ•¸æ‰¾åˆ°å°æ‡‰çš„ ']'
            depth = 0
            pos = bracket_start
            while pos < len(html):
                ch = html[pos]
                if ch == "[":
                    depth += 1
                elif ch == "]":
                    depth -= 1
                    if depth == 0:
                        break
                elif ch == '"':
                    # è·³éå­—ä¸²å…§å®¹
                    pos += 1
                    while pos < len(html) and html[pos] != '"':
                        if html[pos] == "\\":
                            pos += 1  # è·³éè·³è„«å­—å…ƒ
                        pos += 1
                pos += 1

            if depth != 0:
                continue

            array_str = html[bracket_start : pos + 1]

            try:
                items = json_mod.loads(array_str)
            except json_mod.JSONDecodeError:
                continue

            for item in items:
                post = item.get("post", {})
                if not post:
                    continue

                code = post.get("code", "")
                if code in seen_codes:
                    continue
                seen_codes.add(code)

                parsed = self._parse_ssr_post(post, fallback_username)
                if parsed:
                    all_posts.append(parsed)

        return all_posts

    def _parse_ssr_post(
        self, post_data: dict, fallback_username: str = "unknown"
    ) -> Optional[ThreadPost]:
        """
        è§£æ Googlebot SSR ä¸­å–®ä¸€è²¼æ–‡çš„ JSONã€‚
        çµæ§‹èˆ‡ MetaThreads API é¡ä¼¼ï¼Œä½†æ¬„ä½å¯èƒ½ç•¥æœ‰ä¸åŒã€‚

        Args:
            post_data: SSR JSON ä¸­çš„ post ç‰©ä»¶
            fallback_username: å‚™ç”¨ä½¿ç”¨è€…åç¨±

        Returns:
            ThreadPost æˆ– None
        """
        try:
            post_id = str(
                post_data.get("code")
                or post_data.get("pk")
                or post_data.get("id")
                or ""
            )

            # ä½¿ç”¨è€…è³‡è¨Š
            user_data = post_data.get("user", {})
            username = user_data.get("username") or fallback_username

            # æ–‡å­—å…§å®¹
            caption = post_data.get("caption")
            text_content = ""
            if isinstance(caption, dict):
                text_content = caption.get("text", "")
            elif isinstance(caption, str):
                text_content = caption

            # æ™‚é–“æˆ³
            timestamp = None
            taken_at = post_data.get("taken_at")
            if taken_at:
                try:
                    timestamp = datetime.fromtimestamp(int(taken_at))
                except (ValueError, TypeError, OSError):
                    pass

            # äº’å‹•æ•¸æ“š
            like_count = post_data.get("like_count", 0) or 0
            reply_count = (
                post_data.get("text_post_app_info", {}).get("direct_reply_count")
                or post_data.get("reply_count")
                or 0
            )

            # åª’é«”
            media_list: List[ThreadsMedia] = []
            carousel_media = post_data.get("carousel_media", [])

            if carousel_media:
                for media in carousel_media:
                    if media.get("video_versions"):
                        best_video = media["video_versions"][0]
                        media_list.append(ThreadsMedia(
                            url=best_video["url"],
                            media_type="video",
                        ))
                    elif media.get("image_versions2", {}).get("candidates"):
                        best_img = media["image_versions2"]["candidates"][0]
                        media_list.append(ThreadsMedia(
                            url=best_img["url"],
                            media_type="image",
                        ))
            elif post_data.get("video_versions"):
                best_video = post_data["video_versions"][0]
                media_list.append(ThreadsMedia(
                    url=best_video["url"],
                    media_type="video",
                ))
            elif post_data.get("image_versions2", {}).get("candidates"):
                best_img = post_data["image_versions2"]["candidates"][0]
                media_list.append(ThreadsMedia(
                    url=best_img["url"],
                    media_type="image",
                ))

            return ThreadPost(
                id=post_id,
                author_username=username,
                text_content=text_content,
                timestamp=timestamp,
                like_count=like_count,
                reply_count=reply_count,
                media=media_list,
            )

        except Exception as e:
            logger.warning(f"è§£æ SSR è²¼æ–‡å¤±æ•—: {e}")
            return None

    def validate_url(self, url: str) -> bool:
        """é©—è­‰æ˜¯å¦ç‚ºæœ‰æ•ˆçš„ Threads é€£çµ"""
        for pattern in self.THREADS_URL_PATTERNS:
            if re.match(pattern, url):
                return True
        return False

    def extract_post_id(self, url: str) -> Optional[str]:
        """
        å¾ URL æå–è²¼æ–‡ ID

        Args:
            url: Threads é€£çµ

        Returns:
            è²¼æ–‡ ID æˆ– None
        """
        # æ ¼å¼ 1: https://threads.net/@username/post/ABC123xyz æˆ– threads.com
        match = re.match(
            r"https?://(?:www\.)?threads\.(?:net|com)/@[\w.]+/post/([A-Za-z0-9_-]+)",
            url
        )
        if match:
            return match.group(1)

        # æ ¼å¼ 2: https://threads.net/t/ABC123xyz æˆ– threads.com
        match = re.match(
            r"https?://(?:www\.)?threads\.(?:net|com)/t/([A-Za-z0-9_-]+)",
            url
        )
        if match:
            return match.group(1)

        return None

    def extract_username(self, url: str) -> Optional[str]:
        """
        å¾ URL æå–ä½¿ç”¨è€…åç¨±

        Args:
            url: Threads é€£çµ

        Returns:
            ä½¿ç”¨è€…åç¨±æˆ– None
        """
        match = re.match(
            r"https?://(?:www\.)?threads\.(?:net|com)/@([\w.]+)/post/",
            url
        )
        if match:
            return match.group(1)
        return None

    async def download(self, url: str) -> ThreadsDownloadResult:
        """
        ä¸‹è¼‰ Threads è²¼æ–‡å…§å®¹

        Args:
            url: Threads é€£çµ

        Returns:
            ThreadsDownloadResult: ä¸‹è¼‰çµæœ
        """
        if not self.validate_url(url):
            return ThreadsDownloadResult(
                success=False,
                error_message="ç„¡æ³•è§£ææ­¤é€£çµï¼Œè«‹ç¢ºèªæ˜¯å¦ç‚ºæœ‰æ•ˆçš„ Threads é€£çµ",
            )

        post_id = self.extract_post_id(url)
        if not post_id:
            return ThreadsDownloadResult(
                success=False,
                error_message="ç„¡æ³•å¾é€£çµæå–è²¼æ–‡ ID",
            )

        try:
            # åœ¨åŸ·è¡Œç·’æ± ä¸­åŸ·è¡Œï¼ˆMetaThreads æ˜¯åŒæ­¥çš„ï¼‰
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None, self._download_sync, post_id, url
            )
            return result

        except Exception as e:
            logger.error(f"ä¸‹è¼‰ Threads è²¼æ–‡å¤±æ•—: {e}")
            return ThreadsDownloadResult(
                success=False,
                error_message=f"ä¸‹è¼‰å¤±æ•—: {str(e)}",
            )

    def _download_sync(self, post_id: str, url: str) -> ThreadsDownloadResult:
        """åŒæ­¥ä¸‹è¼‰æ–¹æ³•"""
        api_failed = False
        api_error_message = ""
        
        try:
            api = self._get_api()

            # å–å¾—è²¼æ–‡è³‡æ–™
            logger.info(f"æ­£åœ¨æŠ“å– Threads è²¼æ–‡: {post_id}")

            try:
                post_data = api.get_thread(post_id)
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºéŒ¯èª¤å›æ‡‰
                if isinstance(post_data, dict):
                    if post_data.get("status") == "fail" or post_data.get("message"):
                        error_msg = post_data.get("message", "").lower()
                        if "login" in error_msg:
                            api_failed = True
                            api_error_message = "API éœ€è¦ç™»å…¥"
                        elif "not found" in error_msg or "404" in str(post_data):
                            return ThreadsDownloadResult(
                                success=False,
                                error_message="æ‰¾ä¸åˆ°æ­¤è²¼æ–‡ï¼Œå¯èƒ½å·²è¢«åˆªé™¤æˆ–ç‚ºç§äººå…§å®¹",
                            )
                        else:
                            api_failed = True
                            api_error_message = post_data.get('message', 'æœªçŸ¥éŒ¯èª¤')
                            
            except Exception as e:
                error_msg = str(e).lower()
                if "not found" in error_msg or "404" in error_msg:
                    return ThreadsDownloadResult(
                        success=False,
                        error_message="æ‰¾ä¸åˆ°æ­¤è²¼æ–‡ï¼Œå¯èƒ½å·²è¢«åˆªé™¤æˆ–ç‚ºç§äººå…§å®¹",
                    )
                api_failed = True
                api_error_message = str(e)

            # å¦‚æœ API å¤±æ•—ï¼Œå˜—è©¦ Googlebot SSR â†’ Web Scraping
            if api_failed:
                logger.warning(f"MetaThreads API å¤±æ•— ({api_error_message})ï¼Œå˜—è©¦ Googlebot SSR...")

                # å„ªå…ˆå˜—è©¦ Googlebot SSRï¼ˆå¯æ­£ç¢ºè¾¨è­˜ä¸²æ–‡ï¼‰
                ssr_result = self._download_via_googlebot_ssr(url)
                if ssr_result and ssr_result.success:
                    logger.info(f"âœ… Googlebot SSR æˆåŠŸ")
                    return ssr_result

                # Googlebot SSR ä¹Ÿå¤±æ•—ï¼Œé€€å› Web Scraping
                logger.warning("Googlebot SSR å¤±æ•—ï¼Œå˜—è©¦å‚³çµ± Web Scraping...")
                scraped_post = self._download_via_web_scraping(url)
                if scraped_post:
                    logger.info(f"âœ… Web Scraping æˆåŠŸ: @{scraped_post.author_username}")
                    return ThreadsDownloadResult(
                        success=True,
                        content_type="single_post",
                        post=scraped_post,
                    )
                else:
                    return ThreadsDownloadResult(
                        success=False,
                        error_message=f"APIã€Googlebot SSR å’Œ Web Scraping éƒ½ç„¡æ³•å–å¾—å…§å®¹ã€‚\n\nAPI éŒ¯èª¤: {api_error_message}\n\nè«‹ç¢ºèªï¼š\n1. cookies.txt åŒ…å«æœ‰æ•ˆçš„ Instagram ç™»å…¥è³‡è¨Š\n2. æˆ–åœ¨ .env è¨­å®š THREADS_USERNAME å’Œ THREADS_PASSWORD",
                    )

            # è§£æè²¼æ–‡è³‡æ–™
            parent_post = self._parse_post_data(post_data)

            if not parent_post:
                # API å›å‚³ä½†è§£æå¤±æ•—ï¼Œå˜—è©¦ Googlebot SSR â†’ Web Scraping
                logger.warning("API è³‡æ–™è§£æå¤±æ•—ï¼Œå˜—è©¦ Googlebot SSR...")
                ssr_result = self._download_via_googlebot_ssr(url)
                if ssr_result and ssr_result.success:
                    return ssr_result

                logger.warning("Googlebot SSR ä¹Ÿå¤±æ•—ï¼Œå˜—è©¦å‚³çµ± Web Scraping...")
                scraped_post = self._download_via_web_scraping(url)
                if scraped_post:
                    return ThreadsDownloadResult(
                        success=True,
                        content_type="single_post",
                        post=scraped_post,
                    )
                return ThreadsDownloadResult(
                    success=False,
                    error_message="ç„¡æ³•è§£æè²¼æ–‡è³‡æ–™",
                )

            # API æˆåŠŸå–å¾—å–®å‰‡è²¼æ–‡ï¼Œä½†å¯èƒ½æ˜¯ä¸²æ–‡çš„ä¸€éƒ¨åˆ†
            # å˜—è©¦ Googlebot SSR æª¢æŸ¥æ˜¯å¦æœ‰æ›´å¤šä½œè€…çš„åŒä¸²è²¼æ–‡
            ssr_result = self._download_via_googlebot_ssr(url)
            if ssr_result and ssr_result.success:
                if ssr_result.content_type == "thread" and len(ssr_result.thread_posts) > 1:
                    logger.info(
                        f"âœ… Googlebot SSR åµæ¸¬åˆ°ä¸²æ–‡ ({len(ssr_result.thread_posts)} å‰‡)ï¼Œ"
                        f"å„ªå…ˆä½¿ç”¨ SSR çµæœ"
                    )
                    return ssr_result
                # SSR ä¹Ÿæ˜¯å–®å‰‡è²¼æ–‡ï¼Œä½¿ç”¨ API çµæœï¼ˆé€šå¸¸è³‡æ–™æ›´å®Œæ•´ï¼‰
                logger.debug("Googlebot SSR ä¹Ÿæ˜¯å–®å‰‡è²¼æ–‡ï¼Œä½¿ç”¨ API çµæœ")

            # å¦‚æœå•Ÿç”¨äº†æŠ“å–å›è¦†ï¼Œå˜—è©¦å–å¾—å°è©±ä¸²
            if settings.threads_fetch_replies:
                try:
                    conversation = self._fetch_conversation(api, post_id, parent_post)
                    if conversation and conversation.replies:
                        logger.info(f"æˆåŠŸæŠ“å–å°è©±ä¸²ï¼Œå…± {len(conversation.replies)} å‰‡å›è¦†")
                        return ThreadsDownloadResult(
                            success=True,
                            content_type="thread_conversation",
                            conversation=conversation,
                        )
                except Exception as e:
                    logger.warning(f"æŠ“å–å°è©±ä¸²å¤±æ•—ï¼Œå°‡åªå›å‚³åŸå§‹è²¼æ–‡: {e}")

            # å›å‚³å–®ä¸€è²¼æ–‡
            logger.info(f"æˆåŠŸæŠ“å– Threads è²¼æ–‡: @{parent_post.author_username}")
            return ThreadsDownloadResult(
                success=True,
                content_type="single_post",
                post=parent_post,
            )

        except RuntimeError as e:
            # MetaThreads æœªå®‰è£
            return ThreadsDownloadResult(
                success=False,
                error_message=str(e),
            )
        except Exception as e:
            error_msg = str(e)
            logger.error(f"Threads ä¸‹è¼‰å¤±æ•—: {error_msg}")

            if "rate limit" in error_msg.lower():
                return ThreadsDownloadResult(
                    success=False,
                    error_message="å·²é”åˆ° API è«‹æ±‚é™åˆ¶ï¼Œè«‹ç¨å¾Œå†è©¦",
                )

            return ThreadsDownloadResult(
                success=False,
                error_message=f"ä¸‹è¼‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {error_msg}",
            )

    def _parse_post_data(self, post_data: dict) -> Optional[ThreadPost]:
        """
        è§£æ MetaThreads å›å‚³çš„è²¼æ–‡è³‡æ–™

        Args:
            post_data: MetaThreads API å›å‚³çš„åŸå§‹è³‡æ–™

        Returns:
            ThreadPost æˆ– None
        """
        try:
            # MetaThreads çš„è³‡æ–™çµæ§‹å¯èƒ½åœ¨ä¸åŒç‰ˆæœ¬æœ‰æ‰€ä¸åŒ
            # å˜—è©¦å¤šç¨®æ¬„ä½åç¨±
            post_id = (
                post_data.get("id")
                or post_data.get("pk")
                or post_data.get("code")
                or ""
            )

            # å–å¾—ä½œè€…è³‡è¨Š
            user_data = post_data.get("user", {})
            author_username = (
                user_data.get("username")
                or post_data.get("username")
                or "unknown"
            )

            # å–å¾—æ–‡å­—å…§å®¹
            text_content = (
                post_data.get("caption", {}).get("text")
                if isinstance(post_data.get("caption"), dict)
                else post_data.get("caption")
                or post_data.get("text")
                or post_data.get("text_post_app_info", {}).get("share_info", {}).get("quoted_text")
                or ""
            )

            # å–å¾—æ™‚é–“æˆ³è¨˜
            timestamp = None
            taken_at = post_data.get("taken_at") or post_data.get("created_at")
            if taken_at:
                try:
                    if isinstance(taken_at, (int, float)):
                        timestamp = datetime.fromtimestamp(taken_at)
                    elif isinstance(taken_at, str):
                        timestamp = datetime.fromisoformat(taken_at.replace("Z", "+00:00"))
                except Exception:
                    pass

            # å–å¾—äº’å‹•æ•¸æ“š
            like_count = (
                post_data.get("like_count")
                or post_data.get("likes", {}).get("count")
                or 0
            )
            reply_count = (
                post_data.get("reply_count")
                or post_data.get("text_post_app_info", {}).get("direct_reply_count")
                or 0
            )

            # å–å¾—åª’é«” URLï¼ˆå«é¡å‹åˆ¤æ–·ï¼‰
            media_list: List[ThreadsMedia] = []
            carousel_media = post_data.get("carousel_media", [])
            if carousel_media:
                for media in carousel_media:
                    if media.get("video_versions"):
                        media_list.append(ThreadsMedia(
                            url=media["video_versions"][0]["url"],
                            media_type="video"
                        ))
                    elif media.get("image_versions2", {}).get("candidates"):
                        media_list.append(ThreadsMedia(
                            url=media["image_versions2"]["candidates"][0]["url"],
                            media_type="image"
                        ))
            elif post_data.get("video_versions"):
                media_list.append(ThreadsMedia(
                    url=post_data["video_versions"][0]["url"],
                    media_type="video"
                ))
            elif post_data.get("image_versions2", {}).get("candidates"):
                media_list.append(ThreadsMedia(
                    url=post_data["image_versions2"]["candidates"][0]["url"],
                    media_type="image"
                ))

            # å–å¾—å¼•ç”¨è²¼æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰
            quoted_post = None
            quoted_post_data = post_data.get("text_post_app_info", {}).get("share_info", {}).get("quoted_post")
            if quoted_post_data:
                quoted_post = self._parse_post_data(quoted_post_data)

            return ThreadPost(
                id=str(post_id),
                author_username=author_username,
                text_content=text_content,
                timestamp=timestamp,
                like_count=like_count,
                reply_count=reply_count,
                media=media_list,
                quoted_post=quoted_post,
            )

        except Exception as e:
            logger.warning(f"è§£æè²¼æ–‡è³‡æ–™å¤±æ•—: {e}")
            return None

    def _fetch_conversation(
        self,
        api,
        post_id: str,
        parent_post: ThreadPost
    ) -> Optional[ThreadConversation]:
        """
        å–å¾—å®Œæ•´å°è©±ä¸²ï¼ˆåŒ…å«å›è¦†ï¼‰

        Args:
            api: MetaThreads API å¯¦ä¾‹
            post_id: è²¼æ–‡ ID
            parent_post: çˆ¶è²¼æ–‡

        Returns:
            ThreadConversation æˆ– None
        """
        try:
            # å–å¾—å›è¦†
            replies_data = api.get_thread_replies(post_id)

            if not replies_data:
                return ThreadConversation(parent_post=parent_post, replies=[])

            replies = []
            max_replies = settings.threads_max_replies

            for reply_data in replies_data[:max_replies]:
                reply = self._parse_post_data(reply_data)
                if reply:
                    replies.append(reply)

            return ThreadConversation(
                parent_post=parent_post,
                replies=replies,
            )

        except Exception as e:
            logger.warning(f"å–å¾—å°è©±ä¸²å¤±æ•—: {e}")
            return None

    def format_for_summary(self, result: ThreadsDownloadResult) -> str:
        """
        å°‡ä¸‹è¼‰çµæœæ ¼å¼åŒ–ç‚ºé©åˆ LLM æ‘˜è¦çš„æ–‡å­—

        Args:
            result: ä¸‹è¼‰çµæœ

        Returns:
            æ ¼å¼åŒ–å¾Œçš„æ–‡å­—å…§å®¹
        """
        if not result.success:
            return ""

        lines = []

        if result.content_type == "single_post" and result.post:
            lines.append(self._format_post(result.post, is_main=True))

        elif result.content_type == "thread" and result.thread_posts:
            # ä¸²æ–‡ï¼šä½œè€…çš„å¤šå‰‡é€£çºŒè²¼æ–‡
            total = len(result.thread_posts)
            author = result.thread_posts[0].author_username
            lines.append(f"ã€ä¸²æ–‡ã€‘ @{author}ï¼ˆå…± {total} å‰‡ï¼‰")
            for i, post in enumerate(result.thread_posts, 1):
                lines.append(f"\n--- ã€ä¸²æ–‡ {i}/{total}ã€‘ ---")
                lines.append(self._format_post(post, is_main=(i == 1)))

        elif result.content_type == "thread_conversation" and result.conversation:
            # æ ¼å¼åŒ–ä¸»è²¼æ–‡
            lines.append(self._format_post(result.conversation.parent_post, is_main=True))

            # æ ¼å¼åŒ–å›è¦†
            if result.conversation.replies:
                lines.append("\nã€å°è©±ä¸²å›è¦†ã€‘")
                for i, reply in enumerate(result.conversation.replies, 1):
                    lines.append(f"\n--- å›è¦† #{i} ---")
                    lines.append(self._format_post(reply, is_main=False))

        return "\n".join(lines)

    def _format_post(self, post: ThreadPost, is_main: bool = False) -> str:
        """æ ¼å¼åŒ–å–®ä¸€è²¼æ–‡"""
        lines = []

        if is_main:
            lines.append(f"ã€ä¸»è²¼æ–‡ã€‘ @{post.author_username}")
        else:
            lines.append(f"@{post.author_username}")

        if post.timestamp:
            lines.append(f"ç™¼ä½ˆæ™‚é–“: {post.timestamp.strftime('%Y-%m-%d %H:%M')}")

        lines.append(f"\n{post.text_content}")

        if post.like_count > 0 or post.reply_count > 0:
            stats = []
            if post.like_count > 0:
                stats.append(f"â¤ï¸ {post.like_count}")
            if post.reply_count > 0:
                stats.append(f"ğŸ’¬ {post.reply_count}")
            lines.append(f"\n({' | '.join(stats)})")

        if post.quoted_post:
            lines.append(f"\n> å¼•ç”¨è‡ª @{post.quoted_post.author_username}:")
            lines.append(f"> {post.quoted_post.text_content[:200]}...")

        if post.media:
            image_count = sum(1 for m in post.media if m.media_type == "image")
            video_count = sum(1 for m in post.media if m.media_type == "video")
            media_info = []
            if image_count > 0:
                media_info.append(f"{image_count} å¼µåœ–ç‰‡")
            if video_count > 0:
                media_info.append(f"{video_count} å€‹å½±ç‰‡")
            lines.append(f"\n[é™„ä»¶: {', '.join(media_info)}]")

        return "\n".join(lines)

    # ==================== åª’é«”ä¸‹è¼‰æ–¹æ³• ====================

    def _get_temp_dir(self) -> Path:
        """å–å¾—æš«å­˜ç›®éŒ„"""
        temp_dir = Path(settings.temp_video_dir)
        temp_dir.mkdir(parents=True, exist_ok=True)
        return temp_dir

    def _download_image_sync(self, url: str, retry: int = 2) -> Optional[Path]:
        """
        ä¸‹è¼‰å–®å¼µåœ–ç‰‡ï¼ˆåŒæ­¥æ–¹æ³•ï¼Œå«é‡è©¦æ©Ÿåˆ¶ï¼‰

        Args:
            url: åœ–ç‰‡ URL
            retry: é‡è©¦æ¬¡æ•¸

        Returns:
            åœ–ç‰‡æª”æ¡ˆè·¯å¾‘æˆ– None
        """
        temp_dir = self._get_temp_dir()
        file_id = uuid.uuid4().hex[:8]
        image_path = temp_dir / f"threads_img_{file_id}.jpg"

        for attempt in range(retry + 1):
            try:
                response = requests.get(url, timeout=30, stream=True)
                response.raise_for_status()

                with open(image_path, "wb") as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)

                logger.info(f"âœ… åœ–ç‰‡ä¸‹è¼‰æˆåŠŸ: {image_path.name}")
                return image_path

            except Exception as e:
                if attempt < retry:
                    logger.warning(f"åœ–ç‰‡ä¸‹è¼‰å¤±æ•— (ç¬¬ {attempt + 1} æ¬¡)ï¼Œé‡è©¦ä¸­: {e}")
                else:
                    logger.error(f"åœ–ç‰‡ä¸‹è¼‰å¤±æ•— (å·²é”æœ€å¤§é‡è©¦): {e}")

        return None

    def _download_video_sync(self, url: str, retry: int = 2) -> tuple[Optional[Path], Optional[Path]]:
        """
        ä¸‹è¼‰å½±ç‰‡ä¸¦æå–éŸ³è¨Šï¼ˆåŒæ­¥æ–¹æ³•ï¼Œå«é‡è©¦æ©Ÿåˆ¶ï¼‰

        Args:
            url: å½±ç‰‡ URL
            retry: é‡è©¦æ¬¡æ•¸

        Returns:
            (å½±ç‰‡è·¯å¾‘, éŸ³è¨Šè·¯å¾‘) æˆ– (None, None)
        """
        temp_dir = self._get_temp_dir()
        file_id = uuid.uuid4().hex[:8]
        video_path = temp_dir / f"threads_vid_{file_id}.mp4"
        audio_path = temp_dir / f"threads_aud_{file_id}.mp3"

        for attempt in range(retry + 1):
            try:
                # ä¸‹è¼‰å½±ç‰‡
                response = requests.get(url, timeout=60, stream=True)
                response.raise_for_status()

                with open(video_path, "wb") as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)

                logger.info(f"âœ… å½±ç‰‡ä¸‹è¼‰æˆåŠŸ: {video_path.name}")

                # ä½¿ç”¨ ffmpeg æå–éŸ³è¨Šï¼ˆå¦‚æœæœ‰éŸ³è»Œï¼‰
                if self._has_audio_track(video_path):
                    try:
                        subprocess.run(
                            [
                                "ffmpeg", "-y", "-i", str(video_path),
                                "-vn", "-acodec", "libmp3lame", "-q:a", "2",
                                str(audio_path)
                            ],
                            capture_output=True,
                            timeout=60,
                        )
                        if audio_path.exists():
                            logger.info(f"âœ… éŸ³è¨Šæå–æˆåŠŸ: {audio_path.name}")
                            return video_path, audio_path
                    except Exception as e:
                        logger.warning(f"éŸ³è¨Šæå–å¤±æ•—: {e}")

                return video_path, None

            except Exception as e:
                if attempt < retry:
                    logger.warning(f"å½±ç‰‡ä¸‹è¼‰å¤±æ•— (ç¬¬ {attempt + 1} æ¬¡)ï¼Œé‡è©¦ä¸­: {e}")
                else:
                    logger.error(f"å½±ç‰‡ä¸‹è¼‰å¤±æ•— (å·²é”æœ€å¤§é‡è©¦): {e}")

        return None, None

    def _has_audio_track(self, video_path: Path) -> bool:
        """
        ä½¿ç”¨ ffprobe æª¢æ¸¬å½±ç‰‡æ˜¯å¦å«æœ‰éŸ³è»Œ

        Args:
            video_path: å½±ç‰‡æª”æ¡ˆè·¯å¾‘

        Returns:
            True è¡¨ç¤ºæœ‰éŸ³è»Œï¼ŒFalse è¡¨ç¤ºç„¡éŸ³è»Œ
        """
        try:
            result = subprocess.run(
                [
                    "ffprobe", "-v", "error",
                    "-select_streams", "a",
                    "-show_entries", "stream=codec_type",
                    "-of", "csv=p=0",
                    str(video_path)
                ],
                capture_output=True,
                text=True,
                timeout=10,
            )
            has_audio = "audio" in result.stdout.lower()
            logger.debug(f"å½±ç‰‡ {video_path.name} éŸ³è»Œåµæ¸¬: {'æœ‰' if has_audio else 'ç„¡'}")
            return has_audio
        except Exception as e:
            logger.warning(f"éŸ³è»Œåµæ¸¬å¤±æ•—ï¼Œé è¨­ç‚ºæœ‰éŸ³è»Œ: {e}")
            return True  # åµæ¸¬å¤±æ•—æ™‚é è¨­ç‚ºæœ‰éŸ³è»Œï¼ˆæœƒå˜—è©¦è½‰éŒ„ï¼‰

    async def download_media(self, media_list: List[ThreadsMedia]) -> ThreadsMediaDownloadResult:
        """
        ä¸‹è¼‰æ‰€æœ‰åª’é«”æª”æ¡ˆ

        Args:
            media_list: åª’é«”åˆ—è¡¨

        Returns:
            ThreadsMediaDownloadResult: ä¸‹è¼‰çµæœ
        """
        if not media_list:
            return ThreadsMediaDownloadResult(success=True)

        image_paths: List[Path] = []
        video_paths: List[Path] = []
        audio_paths: List[Path] = []

        loop = asyncio.get_event_loop()

        for media in media_list:
            if media.media_type == "image":
                path = await loop.run_in_executor(
                    None, self._download_image_sync, media.url
                )
                if path:
                    image_paths.append(path)

            elif media.media_type == "video":
                video_path, audio_path = await loop.run_in_executor(
                    None, self._download_video_sync, media.url
                )
                if video_path:
                    video_paths.append(video_path)
                if audio_path:
                    audio_paths.append(audio_path)

        # åªè¦æœ‰ä»»ä½•åª’é«”æˆåŠŸä¸‹è¼‰å°±ç®—æˆåŠŸ
        success = len(image_paths) > 0 or len(video_paths) > 0
        error_message = None

        if not success and media_list:
            error_message = "æ‰€æœ‰åª’é«”ä¸‹è¼‰å¤±æ•—"

        logger.info(
            f"åª’é«”ä¸‹è¼‰å®Œæˆ: {len(image_paths)} å¼µåœ–ç‰‡, "
            f"{len(video_paths)} å€‹å½±ç‰‡, {len(audio_paths)} å€‹éŸ³è¨Š"
        )

        return ThreadsMediaDownloadResult(
            success=success,
            image_paths=image_paths,
            video_paths=video_paths,
            audio_paths=audio_paths,
            error_message=error_message,
        )

    def cleanup_media(self, result: ThreadsMediaDownloadResult) -> None:
        """
        æ¸…ç†æš«å­˜çš„åª’é«”æª”æ¡ˆ

        Args:
            result: åª’é«”ä¸‹è¼‰çµæœ
        """
        all_paths = result.image_paths + result.video_paths + result.audio_paths

        for path in all_paths:
            try:
                if path.exists():
                    path.unlink()
                    logger.debug(f"å·²åˆªé™¤æš«å­˜æª”æ¡ˆ: {path.name}")
            except Exception as e:
                logger.warning(f"åˆªé™¤æš«å­˜æª”æ¡ˆå¤±æ•— {path}: {e}")

    def get_all_media(self, result: ThreadsDownloadResult) -> List[ThreadsMedia]:
        """
        å¾ä¸‹è¼‰çµæœä¸­æ”¶é›†æ‰€æœ‰åª’é«”ï¼ˆåŒ…å«ä¸»è²¼æ–‡å’Œå›è¦†ï¼‰

        Args:
            result: Threads ä¸‹è¼‰çµæœ

        Returns:
            æ‰€æœ‰åª’é«”åˆ—è¡¨
        """
        all_media: List[ThreadsMedia] = []

        if result.content_type == "single_post" and result.post:
            all_media.extend(result.post.media)

        elif result.content_type == "thread" and result.thread_posts:
            for post in result.thread_posts:
                all_media.extend(post.media)

        elif result.content_type == "thread_conversation" and result.conversation:
            all_media.extend(result.conversation.parent_post.media)
            for reply in result.conversation.replies:
                all_media.extend(reply.media)

        return all_media
